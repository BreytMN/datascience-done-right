{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upskilling a no-skill classifier with Conformal Prediction\n",
    "\n",
    "Whenever a scientist needs to build a model, they need to evaluate the results against certain metrics given some context.\n",
    "\n",
    "This led to a culture that focus too much on optimizing metrics instead of measuring how the model would impact the business.\n",
    "\n",
    "In this notebook I'll show a way to evaluate models through different lens by using conformal prediction that can helps you given a clear picture of what your model is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from crepes import WrapClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.charts import display_static_altair_images\n",
    "from utils.dataframes import make_classification_df\n",
    "\n",
    "\n",
    "def calculate_coverage(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    label: int,\n",
    "    calibrated_conformal_classifier: WrapClassifier,\n",
    "    alphas: list[float],\n",
    ") -> pd.DataFrame:\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    sets = {\n",
    "        alpha: calibrated_conformal_classifier.predict_set(X, confidence=1 - alpha)\n",
    "        for alpha in alphas\n",
    "    }\n",
    "\n",
    "    count_true_label = sum(True for value in y if value == label)\n",
    "    size = len(y)\n",
    "\n",
    "    random_guessing = count_true_label / size\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        count_coverage = 0\n",
    "        count_sets = 0\n",
    "\n",
    "        sets_aux = sets[alpha]\n",
    "        for i, value in enumerate(y):\n",
    "            if sets_aux[i, label]:\n",
    "                count_sets += 1\n",
    "                if value == label:\n",
    "                    count_coverage += 1\n",
    "\n",
    "        denominator_count_sets = count_sets if count_sets > 0 else 1\n",
    "\n",
    "        res = {\n",
    "            \"alpha\": alpha,\n",
    "            \"coverage\": count_coverage,\n",
    "            \"% coverage (recall)\": round(count_coverage * 100 / count_true_label, 2),\n",
    "            \"# sets containing target\": count_sets,\n",
    "            \"% sets containing_target\": round(count_sets * 100 / size, 2),\n",
    "            \"% sets correctly covering target (precision)\": round(\n",
    "                count_coverage * 100 / denominator_count_sets, 2\n",
    "            ),\n",
    "            \"pp gain over random guessing\": round(\n",
    "                ((count_coverage * 100) / denominator_count_sets)\n",
    "                - (random_guessing * 100),\n",
    "                2,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results).set_index(\"alpha\")\n",
    "\n",
    "\n",
    "def train_and_calibrate(\n",
    "    classifier: ClassifierMixin,\n",
    "    X_train: pd.DataFrame,\n",
    "    X_calib: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_calib: pd.Series,\n",
    "    y_test: pd.Series,\n",
    ") -> WrapClassifier:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_preds = classifier.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_preds, zero_division=0))\n",
    "\n",
    "    conformal_classifier = WrapClassifier(classifier)\n",
    "    conformal_classifier.calibrate(\n",
    "        X_calib.reset_index(drop=True), y_calib.reset_index(drop=True), class_cond=True\n",
    "    )\n",
    "\n",
    "    return conformal_classifier\n",
    "\n",
    "\n",
    "start_alpha = 0.05\n",
    "end_alpha = 0.95\n",
    "num_alpha = int((end_alpha + start_alpha) / 0.05) - 1\n",
    "\n",
    "alphas = np.linspace(start_alpha, end_alpha, num_alpha)\n",
    "alphas = [round(i, 2) for i in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"a\", \"b\"]\n",
    "target = \"y\"\n",
    "\n",
    "df_params = {\n",
    "    \"n_samples\": [20000, 1000, 1000],\n",
    "    \"features\": features,\n",
    "    \"centers\": [(0, 0), (3, 0), (1, 2)],\n",
    "    \"cluster_std\": [2, 0.5, 0.8],\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "df = make_classification_df(**df_params)\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(df)\n",
    "    .mark_point(size=10, opacity=0.5, filled=True)\n",
    "    .encode(\n",
    "        alt.X(\"a:Q\").scale(domain=[-8, 8], clamp=True),\n",
    "        alt.Y(\"b:Q\").scale(domain=[-8, 8], clamp=True),\n",
    "        alt.Color(\"y:N\"),\n",
    "    )\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=600,\n",
    "    )\n",
    ")\n",
    "\n",
    "display_static_altair_images(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above one can already imagine that it will be very hard to correctly classify the label 1 while being almost impossible to do the same with label 2.\n",
    "\n",
    "Some would say \"upsampling\" while others would say \"downsampling\". Upsampling is objectively bad, I'm not wasting my time on why it is bad to create artificial data. Downsampling on the other hand is ok, but it adds an extra layer of complexity that can't be avoided if you want correct results.\n",
    "\n",
    "For this case I prefer Conformal Prediction. It's a method that helps you say something like \"in this region of the feature space we expect this probability for each label\" with mathematical grounding.\n",
    "\n",
    "What does this means in practice?\n",
    " * for a targeted campaign this could mean \"anyone from this region is sure to be at least interested in this ad\" (remember that are cases in which we don't even have the capability of attending the whole demand for something, so if we can reduce our spending in marketing campaigns while still guaranteeing we sell the whole stock it is a big win);\n",
    " * on the other hand, if we are talking about fraud detection, we can use a cheaper model to detect anyone slightly suspicious and then send the results to a more sofisticated and expensive model that maybe is a paid API we contracted for our business, which means that we don't waste too many resources trying to detect frauds on most transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.filter(features), df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "X_calib, X_test, y_calib, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=0\n",
    ")\n",
    "\n",
    "train_data = (X_train, X_calib, X_test, y_train, y_calib, y_test)\n",
    "\n",
    "new_df = make_classification_df(**{**df_params, \"random_state\": 1})\n",
    "new_X, new_y = new_df.filter(features), new_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "\n",
    "Training a Logistic Regression Classifier yields a model that can't classify no one beyonds label 0 correctly. However, it isn't a useless model like the metrics make it seems so.\n",
    "\n",
    "Looking at the table calculated on the results of each set whe can see how the model is able to perform at each alpha. This give us the ability to analyze where our model is performing correctly with virtually 100% certainty that the label is correct while showing us at which threshold the models starts to \"fail\".\n",
    "\n",
    "That means even a classically bad classifier can be useful if we don't have anything better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal_logistic_regression_classifier = train_and_calibrate(\n",
    "    LogisticRegression(random_state=0), *train_data\n",
    ")\n",
    "for label in (0, 1, 2):\n",
    "    print(f\"***** Results for label {label} *****\")\n",
    "    display(\n",
    "        calculate_coverage(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            label,\n",
    "            conformal_logistic_regression_classifier,\n",
    "            alphas,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data\n",
    "\n",
    "It also keeps roughly the same results on data that follows the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in (0, 1, 2):\n",
    "    print(f\"***** Results for label {label} *****\")\n",
    "    display(\n",
    "        calculate_coverage(\n",
    "            new_X,\n",
    "            new_y,\n",
    "            label,\n",
    "            conformal_logistic_regression_classifier,\n",
    "            alphas,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Of course the Random Forest Classifier would perform way better than a Logistic Regression Classifier. However it still performs very bad, but the Conformal Prediction on top of it still gives way better control over the results.\n",
    "\n",
    "But hey, you do lose something in the label 0: there is no region with virtual 100% correct labels! There are some cases in which the no skill Logistic Regression Classifier can be more usefull than the smarter Random Forest Classifier, considering the results of the Conformal Prediction on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal_random_forest_classifier = train_and_calibrate(\n",
    "    RandomForestClassifier(random_state=0), *train_data\n",
    ")\n",
    "for label in (0, 1, 2):\n",
    "    print(f\"***** Results for label {label} *****\")\n",
    "    display(\n",
    "        calculate_coverage(\n",
    "            X_test,\n",
    "            y_test,\n",
    "            label,\n",
    "            conformal_random_forest_classifier,\n",
    "            alphas,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on new data\n",
    "\n",
    "Same as the logistic regression classifier, just so we can see it the conformal prediction helpings in many scenarios while confirming that we lost something for label 0 even in new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in (0, 1, 2):\n",
    "    print(f\"***** Results for label {label} *****\")\n",
    "    display(\n",
    "        calculate_coverage(\n",
    "            new_X,\n",
    "            new_y,\n",
    "            label,\n",
    "            conformal_random_forest_classifier,\n",
    "            alphas,\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
